# ğŸ“Š Projet Open Food Facts - Nettoyage et Clustering des DonnÃ©es  

## ğŸ† Objectif  
Ce projet exploite la base de donnÃ©es **Open Food Facts** afin de :  
- âœ… **Nettoyer** et **prÃ©parer** les donnÃ©es efficacement ğŸ“Œ  
- âœ… Appliquer des techniques de **scaling** et dâ€™**encodage** adaptÃ©es ğŸ›ï¸  
- âœ… RÃ©aliser du **clustering** pour identifier des groupes de produits similaires ğŸ”  

---

## ğŸ“‚ Structure du projet  

```
OPENFOODFACT_GROUPPROJECT/
â”‚â”€â”€ data/
â”‚   â”œâ”€â”€ dataset/
â”‚   â”‚   â”œâ”€â”€ en.openfoodfacts.org.products.csv
â”‚   â”‚   â””â”€â”€ sample_10000.csv
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â””â”€â”€ preprocessed_sample_10000.csv
â”‚   â””â”€â”€ results/
â”‚â”€â”€ notebooks/
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py             # Permet de traiter `src` comme un package Python
â”‚   â”œâ”€â”€ data_loading.py         # Chargement des donnÃ©es
â”‚   â”œâ”€â”€ preprocessing.py        # Nettoyage et prÃ©traitement des donnÃ©es
â”‚   â”œâ”€â”€ feature_engineering.py  # CrÃ©ation de nouvelles variables
â”‚   â”œâ”€â”€ train_model.py          # EntraÃ®nement des modÃ¨les de clustering
â”‚   â”œâ”€â”€ evaluate_model.py       # Ã‰valuation des modÃ¨les
â”‚   â”œâ”€â”€ experiment_logger.py    # Gestion des logs et des expÃ©riences
â”‚   â”œâ”€â”€ config.py               # Fichier de configuration du projet
â”‚   â””â”€â”€ run_pipeline.py         # Script principal d'exÃ©cution du pipeline
â”‚â”€â”€ scripts/
â”‚   â””â”€â”€ __init__.py             
â”‚â”€â”€ requirements.txt            # Liste des dÃ©pendances
â”‚â”€â”€ README.md                   # Documentation du projet

## ğŸ‘¥ RÃ©partition des tÃ¢ches  

Le projet a Ã©tÃ© rÃ©alisÃ© en collaboration avec une Ã©quipe de 4 personnes, chacun ayant contribuÃ© Ã  des Ã©tapes spÃ©cifiques de la chaÃ®ne de traitement :  

### ğŸ”„ **Pipeline global**  
- **Eric** : Responsable du dÃ©veloppement global de la pipeline.  
    - ğŸ› ï¸ A Ã©galement pris en charge la majeure partie du **prÃ©traitement des donnÃ©es** :  
        - SÃ©paration des donnÃ©es selon leurs types (numÃ©riques et non-numÃ©riques).  
        - Encodage des donnÃ©es non-numÃ©riques en fonction de leur cardinalitÃ©.  

### ğŸ“‰ **Sous-Ã©chantillonnage des donnÃ©es**  
- **Emilia** : ChargÃ©e de la gÃ©nÃ©ration de diffÃ©rents sous-Ã©chantillons du dataset initial :  
    - ğŸ“Š DÃ©finition de seuils de tolÃ©rance (60% Ã  90%) pour les valeurs contenues dans les Ã©chantillons.  
    - ğŸ§¹ Application de 3 mÃ©thodes de remplacement des valeurs manquantes :  
        - Imputation par **KNN**.  
        - Remplacement par la **moyenne**.  
        - Remplacement par la **mÃ©diane**.  
    - ğŸš¦ Gestion des outliers selon 3 stratÃ©gies :  
        - Suppression des outliers.  
        - Conservation des outliers.  
        - Remplacement des outliers en utilisant la mÃªme mÃ©thode dâ€™imputation que celle appliquÃ©e au dataset.  

### ğŸ§¬ **SÃ©lection des variables**  
- **GrÃ©goire** : Responsable de la **sÃ©lection des features** pertinentes pour le modÃ¨le.  

### ğŸ¤– **EntraÃ®nement et Ã©valuation des modÃ¨les**  
- **Alexandre** : ChargÃ© de lâ€™**entraÃ®nement des modÃ¨les** de clustering et de leur **Ã©valuation**.  

---  
GrÃ¢ce Ã  cette rÃ©partition, chaque membre a pu se concentrer sur une Ã©tape clÃ©, garantissant une progression fluide et collaborative du projet.  
**
**

# ğŸ“Š Projet Open Food Facts - Nettoyage et Clustering des DonnÃ©es  

## ğŸš€ Comment lancer le projet ?

Pour lancer le projet, suivez les Ã©tapes ci-dessous :

1. Installez les dÃ©pendances nÃ©cessaires :
   ```bash
   pip install -r requirements.txt
   ```

2. Configurez les paramÃ¨tres dans le fichier src/config.py. Ce fichier contient toutes les options et paramÃ¨tres nÃ©cessaires pour chaque Ã©tape de la pipeline.

3. ExÃ©cutez la pipeline avec la commande suivante :
```bash
python src/run_pipeline.py
```

ğŸ› ï¸ Pipeline de traitement des donnÃ©es
La pipeline est dÃ©composÃ©e en 7 Ã©tapes principales. Actuellement, seules les 3 premiÃ¨res Ã©tapes sont configurÃ©es :

Ã‰tape 1 : Chargement des donnÃ©es
Chargement des donnÃ©es brutes depuis le chemin spÃ©cifiÃ© dans config.DATA_PATH.
Sous-Ã©chantillonnage des donnÃ©es (par exemple, 10 000 lignes) et sauvegarde dans un fichier CSV.

Ã‰tape 2 : PrÃ©traitement des donnÃ©es
Suppression des colonnes inutiles.
Gestion des valeurs manquantes (imputation par moyenne, mÃ©diane, ou KNN).
Application des paramÃ¨tres dÃ©finis dans config.PREPROCESSING_PARAMS.

Ã‰tape 3 : IngÃ©nierie des features
SÃ©lection des features pertinentes en fonction de la mÃ©thode spÃ©cifiÃ©e dans config.FEATURE_PARAMS.
MÃ©thodes disponibles : correlation, variance, anova_k_best, random_forest, etc.

ğŸ“‹ Exemple de configuration dans src/config.py
Voici un exemple de configuration pour les paramÃ¨tres de la pipeline :

```python
DATA_PATH = "./data/dataset/openfoodfacts.csv"
SAMPLE_PATH = "./data/dataset/sample_10000.csv"
DATA_PROCESSED_PATH = "./data/processed/processed_sample_10000.csv"

PREPROCESSING_PARAMS = {
    "columns_to_drop": ["column1", "column2"],
    "missing_value_threshold": 0.3,
    "imputation_strategy": "mean",
    "knn_neighbors": 5,
}

FEATURE_PARAMS = {
    "method": "correlation",
    "threshold": 0.9,
    "target_column": "target",
    "k": 10,
    "percentile": 10,
    "score_func": None,
    "mode": "percentile",
    "param": 10,
    "threshold_model": "mean",
    "n_features_to_select": 10,
    "direction": "forward",
}
```

ğŸ“ˆ Ã‰tapes futures
Les Ã©tapes suivantes restent Ã  configurer dans la pipeline :

Ã‰tape 4 : Train/Test Split : Division des donnÃ©es en ensembles d'entraÃ®nement et de test.
Ã‰tape 5 : EntraÃ®nement du modÃ¨le : EntraÃ®nement des modÃ¨les de clustering ou de classification.
Ã‰tape 6 : Ã‰valuation du modÃ¨le : Ã‰valuation des performances des modÃ¨les.
Ã‰tape 7 : Sauvegarde des rÃ©sultats : Sauvegarde des modÃ¨les et des rÃ©sultats d'Ã©valuation.