# ğŸ“Š Projet Open Food Facts - Nettoyage et Clustering des DonnÃ©es  

## ğŸ† Objectif  
Ce projet exploite la base de donnÃ©es **Open Food Facts** afin de :  
- âœ… **Nettoyer** et **prÃ©parer** les donnÃ©es efficacement ğŸ“Œ  
- âœ… Appliquer des techniques de **scaling** et dâ€™**encodage** adaptÃ©es ğŸ›ï¸  
- âœ… RÃ©aliser du **clustering** pour identifier des groupes de produits similaires ğŸ”  

---

## ğŸ“‚ Structure du projet  

```
OPENFOODFACT_GROUPPROJECT/
â”‚â”€â”€ data/
â”‚   â”œâ”€â”€ dataset/
â”‚   â”‚   â”œâ”€â”€ en.openfoodfacts.org.products.csv
â”‚   â”‚   â””â”€â”€ sample_10000.csv
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â””â”€â”€ preprocessed_sample_10000.csv
â”‚   â””â”€â”€ results/
â”‚â”€â”€ notebooks/
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py             # Permet de traiter `src` comme un package Python
â”‚   â”œâ”€â”€ data_loading.py         # Chargement des donnÃ©es
â”‚   â”œâ”€â”€ preprocessing.py        # Nettoyage et prÃ©traitement des donnÃ©es
â”‚   â”œâ”€â”€ feature_engineering.py  # CrÃ©ation de nouvelles variables
â”‚   â”œâ”€â”€ train_model.py          # EntraÃ®nement des modÃ¨les de clustering
â”‚   â”œâ”€â”€ evaluate_model.py       # Ã‰valuation des modÃ¨les
â”‚   â”œâ”€â”€ experiment_logger.py    # Gestion des logs et des expÃ©riences
â”‚   â”œâ”€â”€ config.py               # Fichier de configuration du projet
â”‚   â””â”€â”€ run_pipeline.py         # Script principal d'exÃ©cution du pipeline
â”‚â”€â”€ scripts/
â”‚   â””â”€â”€ __init__.py             
â”‚â”€â”€ requirements.txt            # Liste des dÃ©pendances
â”‚â”€â”€ README.md                   # Documentation du projet

## ğŸ‘¥ RÃ©partition des tÃ¢ches  

Le projet a Ã©tÃ© rÃ©alisÃ© en collaboration avec une Ã©quipe de 4 personnes, chacun ayant contribuÃ© Ã  des Ã©tapes spÃ©cifiques de la chaÃ®ne de traitement :  

### ğŸ”„ **Pipeline global**  
- **Eric** : Responsable du dÃ©veloppement global de la pipeline.  
    - ğŸ› ï¸ A Ã©galement pris en charge la majeure partie du **prÃ©traitement des donnÃ©es** :  
        - SÃ©paration des donnÃ©es selon leurs types (numÃ©riques et non-numÃ©riques).  
        - Encodage des donnÃ©es non-numÃ©riques en fonction de leur cardinalitÃ©.  

### ğŸ“‰ **Sous-Ã©chantillonnage des donnÃ©es**  
- **Emilia** : ChargÃ©e de la gÃ©nÃ©ration de diffÃ©rents sous-Ã©chantillons du dataset initial :  
    - ğŸ“Š DÃ©finition de seuils de tolÃ©rance (60% Ã  90%) pour les valeurs contenues dans les Ã©chantillons.  
    - ğŸ§¹ Application de 3 mÃ©thodes de remplacement des valeurs manquantes :  
        - Imputation par **KNN**.  
        - Remplacement par la **moyenne**.  
        - Remplacement par la **mÃ©diane**.  
    - ğŸš¦ Gestion des outliers selon 3 stratÃ©gies :  
        - Suppression des outliers.  
        - Conservation des outliers.  
        - Remplacement des outliers en utilisant la mÃªme mÃ©thode dâ€™imputation que celle appliquÃ©e au dataset.  

### ğŸ§¬ **SÃ©lection des variables**  
- **GrÃ©goire** : Responsable de la **sÃ©lection des features** pertinentes pour le modÃ¨le.  

### ğŸ¤– **EntraÃ®nement et Ã©valuation des modÃ¨les**  
- **Alexandre** : ChargÃ© de lâ€™**entraÃ®nement des modÃ¨les** de clustering et de leur **Ã©valuation**.  

---  
GrÃ¢ce Ã  cette rÃ©partition, chaque membre a pu se concentrer sur une Ã©tape clÃ©, garantissant une progression fluide et collaborative du projet.  
**
**